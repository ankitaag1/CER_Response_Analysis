{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a3deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"CER_reasoning.csv\",engine='python',encoding='cp1252',na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e8515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows where 'Text' column has blanks\n",
    "df = df[df['Text'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87efa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Function to check if a string contains only punctuation\n",
    "def is_only_punctuation(text):\n",
    "    return all(char in string.punctuation for char in text)\n",
    "# Remove rows where 'content' contains only punctuation\n",
    "df = df[~df['Text'].apply(is_only_punctuation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69873fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove special characters and punctuations\n",
    "import regex\n",
    "df['Text'] = df['Text'].str.replace('[^\\w\\s]','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc66525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations, stopwords from 'content' and lemmatize text\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "data = df.Text.values.tolist()\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])\n",
    "import nltk \n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#Add custom stop words in the list; for example stop_words.extend(['student', 'read', 'write'])\n",
    "stop_words.extend([])\n",
    "import spacy \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags and len(token)>3])\n",
    "    return texts_out\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'] )\n",
    "print(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove empty lists\n",
    "filtered_list = [x for x in data_lemmatized if x]\n",
    "# Remove commas and merge each inner list into a single string\n",
    "cleaned_data = [\" \".join(item).replace(\",\", \"\") for item in filtered_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c71c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate unigrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "bag_of_words_unigrams = vectorizer.fit_transform(cleaned_data)\n",
    "vectorizer.vocabulary_\n",
    "sum_words = bag_of_words_unigrams.sum(axis=0) \n",
    "words_freq_uni = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq_uni =sorted(words_freq_uni, key = lambda x: x[1], reverse=True)\n",
    "print (words_freq_uni[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word cloud with unigrams\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "words_dict = dict(words_freq_uni)\n",
    "WC_height = 1000\n",
    "WC_width = 1500\n",
    "WC_max_words = 200\n",
    "wordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width)\n",
    "wordCloud.generate_from_frequencies(words_dict)\n",
    "plt.title('Most frequently occurring unigrams')\n",
    "plt.imshow(wordCloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b621eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "bag_of_words = vectorizer.fit_transform(cleaned_data)\n",
    "vectorizer.vocabulary_\n",
    "sum_words = bag_of_words.sum(axis=0) \n",
    "words_freq_bi = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq_bi =sorted(words_freq_bi, key = lambda x: x[1], reverse=True)\n",
    "print (words_freq_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e105b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word cloud with bigrams\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "words_dict = dict(words_freq_bi)\n",
    "WC_height = 1000\n",
    "WC_width = 1500\n",
    "WC_max_words = 200\n",
    "wordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width)\n",
    "wordCloud.generate_from_frequencies(words_dict)\n",
    "plt.title('Most frequently occurring bigrams connected by same colour and font size')\n",
    "plt.imshow(wordCloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
